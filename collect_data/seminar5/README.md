# Сбор и разметка данных

## Решение задач семинара 5

### Задание


1. Найдите сайт, содержащий интересующий вас список или каталог. Это может быть список книг, фильмов, спортивных команд или что-то еще, что вас заинтересовало.
2. Создайте новый проект Scrapy и определите нового паука. С помощью атрибута start_urls укажите URL выбранной вами веб-страницы.
3. Определите метод парсинга для извлечения интересующих вас данных. Используйте селекторы XPath или CSS для навигации по HTML и извлечения данных. Возможно, потребуется извлечь данные с нескольких страниц или перейти по ссылкам на другие страницы.
4. Сохраните извлеченные данные в структурированном формате. Вы можете использовать оператор yield для возврата данных из паука, которые Scrapy может записать в файл в выбранном вами формате (например, JSON или CSV).
5. Конечным результатом работы должен быть код Scrapy Spider, а также пример выходных данных. Не забывайте соблюдать правила robots.txt и условия обслуживания веб-сайта, а также ответственно подходите к использованию веб-скрейпинга.



### Решение

1. https://www.partsdirect.ru/for_soldering/flux_and_rosin

2. Создание проекта:
* scrapy startproject parts_direct_flux
* cd parts_direct_flux
* scrapy genspider fluxes "www.partsdirect.ru/for_soldering/flux_and_rosin"

3,4,5:
* паук в файле fluxes.py
* запуск паука: **scrapy crawl fluxes** - создает два файла:
а) fluxes_finded.csv - обнаруженные позиции со ссылками
б) fluxes_crawled.csv - извлеченные со страниц данные 
